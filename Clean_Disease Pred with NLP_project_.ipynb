{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHFcuxB754Gg",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DISEASE PREDICTION WITH NLP - OA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data and Necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "cUxvdlIU3yAc",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4f3200c7-73ee-4e84-e16b-7346b09d8905"
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchtext transformers sentencepiece pandas tqdm datasets\n",
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "#!curl https://ftp.ncbi.nlm.nih.gov/pub/lu/Suppl/BioSentVec/BioWordVec_PubMed_MIMICIII_d200.vec.bin\n",
    "# alternatively, do this with a progress bar\n",
    "#!curl  --progress-bar -O https://ftp.ncbi.nlm.nih.gov/pub/lu/Suppl/BioSentVec/BioWordVec_PubMed_MIMICIII_d200.vec.bin\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install torch torchvision torchaudio\n",
    "# Installing SPARQLWrapper to access the disease ontology\n",
    "#!pip install SPARQLWrapper\n",
    "#!pip install ipywidgets\n",
    "#!jupyter nbextension enable --py widgetsnbextension --sys-prefix\"\n",
    "#!pip install gensim\n",
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install transformers\n",
    "#!pip install tf-keras\n",
    "#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_ner_bc5cdr_md-0.5.0.tar.gz\n",
    "#!pip install nlpaug\n",
    "#!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_Sr13jK36E5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('punkt')\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, Conv1D, MaxPooling1D, Flatten, Activation\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification, pipeline\n",
    "from gensim.models import KeyedVectors\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import joblib\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "784p9sc2pVjQ"
   },
   "source": [
    "#### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFGFb-Eb5R83"
   },
   "outputs": [],
   "source": [
    "## Major Datasets\n",
    "nls2d = pd.read_csv(\"Natural language Symptom2Disease prediction.csv\")\n",
    "textsympdata = pd.read_csv(\"full_textbased_symptom_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "ni0JefWbPbxi",
    "outputId": "40f0e414-6e40-4903-9013-a7f3faeb80d8"
   },
   "outputs": [],
   "source": [
    "textsympdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkY8l26DybKj"
   },
   "outputs": [],
   "source": [
    "textsympdata.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1u8a2oZYPMOU",
    "outputId": "fdb9435b-ac1e-464b-b749-3e29fbf6e927"
   },
   "outputs": [],
   "source": [
    "# Choosing only the Disease and description columns\n",
    "nls2d = nls2d[['Disease', 'text']]\n",
    "nls2d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElNA0ssgbIXU",
    "outputId": "f9964d60-f952-4e73-c30e-05ec2ea93510"
   },
   "outputs": [],
   "source": [
    "textsympdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZljP2PUcSVi",
    "outputId": "cf745f90-6128-4512-814f-0d5d50eecb17"
   },
   "outputs": [],
   "source": [
    "nls2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZ_iqrPVo1Bf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Combining the disease symptom and text descriptions datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4CK0AinOL7r",
    "outputId": "74e89216-a0a4-4c57-df47-f520177eb3fb"
   },
   "outputs": [],
   "source": [
    "# Merging the datasets on the Disease column\n",
    "combineddata = pd.merge(nls2d, textsympdata, on=\"Disease\", how=\"outer\")\n",
    "print(combineddata)\n",
    "combineddata.to_csv(\"Combined disease and symptom text data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "ZpY-GEs8KZbW",
    "outputId": "6c28914c-6dc1-445d-df23-8f3e70f424f2"
   },
   "outputs": [],
   "source": [
    "combineddata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2YxClbk5Kcav",
    "outputId": "c9e37218-d3d7-479b-bb18-4c3cc34e6850"
   },
   "outputs": [],
   "source": [
    "combineddata.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eS90QTsWosfV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Understanding which diseases are responsible for the dataset explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZoDUdXAk_TM",
    "outputId": "b0e737e1-54b9-4a30-b2b0-735aebafad55"
   },
   "outputs": [],
   "source": [
    "# Count of rows for each disease in each dataset\n",
    "nls2d_counts = nls2d['Disease'].value_counts()\n",
    "textsympdata_counts = textsympdata['Disease'].value_counts()\n",
    "\n",
    "# Merging the counts into one DataFrame\n",
    "disease_merge_counts = pd.DataFrame({\n",
    "    \"nls2d_count\": nls2d_counts,\n",
    "    \"textsympdata_count\": textsympdata_counts\n",
    "}).fillna(0)\n",
    "\n",
    "# Adding an column for the expected rows\n",
    "disease_merge_counts['expected_rows'] = disease_merge_counts['nls2d_count'] * disease_merge_counts['textsympdata_count']\n",
    "\n",
    "# Summary\n",
    "print(disease_merge_counts)\n",
    "print(f\"Expected total rows after merge: {disease_merge_counts['expected_rows'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsLFERxCvCmL",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Baseline modelling - TFIDF Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2dd0oyGtvIzf",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "42b04752-0680-43f6-99c1-c7e46e924fa6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = combineddata['text'].fillna('')  # Replacing all the NAN values empty strings\n",
    "y = combineddata['Disease']\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "X_train, X_test, y_train, b_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction with TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Logistic regression model with One-vs-Rest classifier for multilabel classification\n",
    "model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(b_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(b_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUnigpAQxawh"
   },
   "outputs": [],
   "source": [
    "# Defining a function to process and predict from user input - For demo\n",
    "def predict_disease(symptom_text, model, tfidf_vectorizer, top_n=5):\n",
    "    \n",
    "    # Converting the input text into a TF-IDF vector\n",
    "    symptom_vector = tfidf_vectorizer.transform([symptom_text])\n",
    "\n",
    "    # Predicting probabilities for each disease\n",
    "    probabilities = model.predict_proba(symptom_vector)[0]  \n",
    "\n",
    "    #  Most likely diseases\n",
    "    top_disease_indices = probabilities.argsort()[-top_n:][::-1]  \n",
    "    top_disease_probs = probabilities[top_disease_indices]  \n",
    "    top_disease_labels = model.classes_[top_disease_indices]  \n",
    "\n",
    "    # Results\n",
    "    predictions = list(zip(top_disease_labels, top_disease_probs))\n",
    "    print(\"Top predicted diseases and probabilities:\")\n",
    "    for disease, prob in predictions:\n",
    "        print(f\"{disease}: {prob:.4f}\")\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QxXpYlWBifY",
    "outputId": "d179c1d0-8a49-4859-cd20-480f8791d79e"
   },
   "outputs": [],
   "source": [
    "# Sample 1: Predictions using training data language\n",
    "Patient_input1 = \"I have a headache and a high fever\"\n",
    "predictions = predict_disease(Patient_input1, model, tfidf, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkaNfvHE2EhF",
    "outputId": "0c82558c-e207-42e8-e792-698ece76702e"
   },
   "outputs": [],
   "source": [
    "# Sample 2 : Using unseen Language\n",
    "patient_input2 = \"I always want to pee, i have a headache and i have a high fever\"\n",
    "predictions = predict_disease(patient_input2, model, tfidf, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8ef_TuuZYYHV",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter as ctr\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ad6ok03LYpMi",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "31360c95-8811-4684-8ec8-229a3aefe2f3"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "collapsed": true,
    "id": "zdNyKgxZYsoz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5be0666b-d164-42cc-d79f-2479451099d2"
   },
   "outputs": [],
   "source": [
    "nls2d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ULTW2QUCYzlw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "685e52f7-2fec-4424-a718-504e54504395"
   },
   "outputs": [],
   "source": [
    "ctr(nls2d['Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "collapsed": true,
    "id": "-rLk0f4AZ4WY",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f34e77f8-90e2-4eb8-fe03-a299c6a66ddc"
   },
   "outputs": [],
   "source": [
    "nls2d.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsFOJG3baHDH"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "  tokens = word_tokenize(text)\n",
    "  snowball_stemmer = SnowballStemmer('english')\n",
    "  tokens = [snowball_stemmer.stem(token.lower()) for token in tokens if token.isalpha()]\n",
    "  return ' '.join(tokens)\n",
    "\n",
    "nls2d[\"text\"] = nls2d[\"text\"].apply(text_preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6xS_NjMbQeP"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "A = tfidf_vectorizer.fit_transform(nls2d[\"text\"])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "b = label_encoder.fit_transform(nls2d[\"Disease\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tt0eP3mbmz-"
   },
   "outputs": [],
   "source": [
    "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=42)\n",
    "\n",
    "base_models = [\n",
    "    (\"nb\", MultinomialNB()),\n",
    "    (\"rf\", RandomForestClassifier()),\n",
    "    (\"lr\", LogisticRegression()),\n",
    "    (\"svm\", SVC(kernel = \"linear\", probability = True))\n",
    "]\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=base_models, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "WswNUtmVcWuR",
    "outputId": "d781822e-2a02-437c-d875-0f79589234c4"
   },
   "outputs": [],
   "source": [
    "voting_classifier.fit(A_train, b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "npTqpnqHcYUV",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "227fa1fd-22c6-4561-bbdd-b5bc260909d6"
   },
   "outputs": [],
   "source": [
    "accuracy = voting_classifier.score(A_test, b_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9EfUBlrzM_h"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(voting_classifier, 'voting_classifier_model_Disease_pred_97_percent_acc.pkl')\n",
    "loaded_model = joblib.load('voting_classifier_model_Disease_pred_97_percent_acc.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oP4qzxcV1AGV",
    "outputId": "a0fcca93-d698-4487-f38a-3bae8f0313ba"
   },
   "outputs": [],
   "source": [
    "# Sample text\n",
    "sample_text = \"I have to pee all the time and I am stressed.\"\n",
    "sample_text_processed = text_preprocessing(sample_text)\n",
    "sample_text_transformed = tfidf_vectorizer.transform([sample_text_processed])\n",
    "predicted_label = label_encoder.inverse_transform(voting_classifier.predict(sample_text_transformed))\n",
    "\n",
    "print(\"Predicted Label:\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTQAKB1UG2Li",
    "outputId": "ae02ba03-4dd5-483b-8f37-a6eb18a45d6c"
   },
   "outputs": [],
   "source": [
    "# Compute accuracy\n",
    "accuracy = accuracy_score(b_test, b_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Compute precision\n",
    "precision = precision_score(b_test, b_test, average='macro')  # 'macro' computes precision for each label, and returns the average\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Compute recall\n",
    "recall = recall_score(b_test, b_test, average='macro')  # 'macro' computes recall for each label, and returns the average\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Compute F1-score\n",
    "f1 = f1_score(b_test, b_test, average='macro')  # 'macro' computes F1-score for each label, and returns the average\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(b_test, b_test)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac7lBOprGxA8"
   },
   "outputs": [],
   "source": [
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer_disease_nlp.joblib')\n",
    "joblib.dump(label_encoder, 'label_encoder_disease_nlp.joblib')\n",
    "\n",
    "import joblib\n",
    "voting_classifier = joblib.load('voting_classifier_model_Disease_pred_97_percent_acc.pkl')\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer_disease_nlp.joblib')\n",
    "label_encoder = joblib.load('label_encoder_disease_nlp.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCAbArjjIOvC",
    "outputId": "44128161-7e3b-43fd-a4ce-4b9f5c802b9f"
   },
   "outputs": [],
   "source": [
    "# full code with everything saved to joblib\n",
    "\n",
    "# Mdel and vectorizer\n",
    "voting_classifier = joblib.load('voting_classifier_model_Disease_pred_97_percent_acc.pkl')\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer_disease_nlp.joblib')\n",
    "label_encoder = joblib.load('label_encoder_disease_nlp.joblib')\n",
    "\n",
    "# Sample text\n",
    "sample_text = \"I have been experiencing a skin rash on my arm for the past few weeks.\"\n",
    "\n",
    "# Preprocess the sample text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    snowball_stemmer = SnowballStemmer('english')\n",
    "    tokens = [snowball_stemmer.stem(token.lower()) for token in tokens if token.isalpha()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "sample_text_processed = preprocess_text(sample_text)\n",
    "\n",
    "# Transform the preprocessed sample text using the loaded vectorizer\n",
    "sample_text_transformed = tfidf_vectorizer.transform([sample_text_processed])\n",
    "\n",
    "# Predict using the loaded model\n",
    "predicted_label_encoded = voting_classifier.predict(sample_text_transformed)\n",
    "\n",
    "# Decode the predicted label\n",
    "predicted_label = label_encoder.inverse_transform(predicted_label_encoded)\n",
    "\n",
    "print(\"Predicted Label:\", predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i42EbvqrRv-X",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Additional Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHXyKnYfTGuu"
   },
   "source": [
    "Checking the combineddata to see which diseases have text descriptions and which don't. I'd also like to see what percentage of the data the ones that have a text based symptom description cover so I'll know if my best approach is just to drop the ones that dont have a text based symptom description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "KBO63Y-MYaJ2",
    "outputId": "41fd2803-3c72-4c16-dc6b-d13564461200"
   },
   "outputs": [],
   "source": [
    "#standardizing the disease names to prevent dropping a disease due to capitalization or space\n",
    "\n",
    "# Replacing NaN with empty strings and ensure all values are strings\n",
    "combineddata = combineddata.fillna('').astype(str)\n",
    "\n",
    "# Standardizing disease names to lowercase and stripping the leading/trailing spaces\n",
    "combineddata['Disease'] = combineddata['Disease'].str.lower().str.strip()\n",
    "\n",
    "# Checking for unique diseases after normalization\n",
    "unique_diseases = combineddata['Disease'].unique()\n",
    "print(f\"Unique diseases after normalization: {len(unique_diseases)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dawNIfLzhqlK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "summary_diseases = combineddata['Disease'].value_counts()\n",
    "print(summary_diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DzpVOayxhnAA",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Mapping diseasese descriptions to empty text spaces\n",
    "disease_text_mapping = combineddata[combineddata['text'].str.strip() != ''].groupby('Disease')['text'].first().to_dict()\n",
    "\n",
    "# Populating the missing text descriptions using the mapping\n",
    "combineddata['text'] = combineddata.apply(\n",
    "    lambda row: disease_text_mapping[row['Disease']] if row['text'].strip() == '' and row['Disease'] in disease_text_mapping else row['text'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjn8fzTVdEih",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Just checking\n",
    "combineddata.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "3rA_g0iYgfcU",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for missing text-based symptom descriptions\n",
    "missing_text = combineddata['text'].str.strip().eq('')\n",
    "\n",
    "# Number of rows with and without text descriptions\n",
    "num_with_text = (~missing_text).sum()\n",
    "num_without_text = missing_text.sum()\n",
    "\n",
    "# Total number of rows\n",
    "total_rows = len(combineddata)\n",
    "\n",
    "# Coverage percentages\n",
    "coverage_with_text = (num_with_text / total_rows) * 100\n",
    "coverage_without_text = (num_without_text / total_rows) * 100\n",
    "\n",
    "# Results\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Rows with text descriptions: {num_with_text} ({coverage_with_text:.2f}%)\")\n",
    "print(f\"Rows without text descriptions: {num_without_text} ({coverage_without_text:.2f}%)\")\n",
    "\n",
    "# Checking for diseases without text descriptions\n",
    "diseases_without_text = combineddata[missing_text]['Disease'].unique()\n",
    "diseases_with_text = combineddata[~missing_text]['Disease'].unique()\n",
    "print(\"Diseases without text descriptions:\", diseases_without_text)\n",
    "print(\"Diseases with text descriptions:\", diseases_with_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u2ZqE56UQQ7"
   },
   "source": [
    "Since 97.44% of the dataset contains rows with text descriptions of the disease, I have decided to drop the rows that do not contain a text description and document which diseases were dropped out of the dataset. This should ease the rest of the process with preprocessing and model building. If at any point later in the advancement of this model, I feel it is necessary to reinclude these diseases, I will check SNOMED-CT or the Disease Oncologyfor standard disease descriptions of the diseases that were dropped or work to collect data on them for the models.\n",
    "\n",
    "The old dataset contains 44 unique diseases\n",
    "\n",
    "The new dataset contains 24 Unique diseases namely: 'Acne' 'Arthritis' 'Bronchial Asthma' 'Cervical spondylosis'\n",
    " 'Chicken pox' 'Common Cold' 'Dengue' 'Dimorphic Hemorrhoids'\n",
    " 'Fungal infection' 'Hypertension' 'Impetigo' 'Jaundice' 'Malaria'\n",
    " 'Migraine' 'Pneumonia' 'Psoriasis' 'Typhoid' 'Varicose Veins' 'allergy'\n",
    " 'diabetes' 'drug reaction' 'gastroesophageal reflux disease'\n",
    " 'peptic ulcer disease' 'urinary tract infection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjTrt72oVKpJ"
   },
   "outputs": [],
   "source": [
    "# Dropping the rows without Text based symptom descriptions\n",
    "combineddata_with_text = combineddata[~missing_text]\n",
    "\n",
    "# Print remaining rows\n",
    "print(f\"Remaining rows after dropping: {len(combineddata_with_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "-m5aN46FVU2h",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "combineddata_with_text.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShQN-WgHt_5B"
   },
   "outputs": [],
   "source": [
    "# Saving the new combined dataset with text to my computer for use in modelling\n",
    "combineddata_with_text.to_csv(\"New combined dataset with text.csv\", index=False)\n",
    "\n",
    "print(\"Dataset saved as 'New combined dataset with text'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnjKf6-kWRzS"
   },
   "outputs": [],
   "source": [
    "\"\"\"# Documenting the dropped rows and diseases for future records and advancements\n",
    "diseases_without_text = combineddata[missing_text]['Disease'].unique()\n",
    "dropped_diseases_df = pd.DataFrame(diseases_without_text, columns=[\"Disease\"])\n",
    "dropped_diseases_df.to_csv(\"All_dropped_diseases.csv\", index=False)\n",
    "\n",
    "print(f\"Dropped diseases saved to 'All dropped_diseases.csv'\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNgVAIo5sfcD"
   },
   "source": [
    "** Notes: \n",
    "\n",
    "New dataset containing 24 unique diseases is now prepared. This new dataset for NER and SNOMED-CT/Disease oncology mapping as well as the final modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nrs-C22Xy6kR",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDSN4sqhOaFJ"
   },
   "source": [
    "#### Loading the updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqe8Jq2Ut3Cm",
    "outputId": "3384b70b-5741-48cc-b029-b746d654b20a"
   },
   "outputs": [],
   "source": [
    "dpnlp = pd.read_csv(\"New combined dataset with text.csv\")\n",
    "\n",
    "# Replace NaN with empty strings and ensure all values are strings\n",
    "dpnlp = dpnlp.fillna('').astype(str)\n",
    "dpnlp.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwJnKVHWN0gX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e98O97ZQNxL0",
    "outputId": "1089a2b6-36ff-446b-9398-6cae9b720762"
   },
   "outputs": [],
   "source": [
    "#  initializing tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Removing punctuation and special characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    # Removing stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Appling preprocessing to the 'text' column\n",
    "dpnlp['processed_text'] = dpnlp['text'].apply(preprocess_text)\n",
    "\n",
    "# Saving the preprocessed dataset\n",
    "dpnlp.to_csv(\"Preprocessed_dataset.csv\", index=False)\n",
    "\n",
    "dpnlp.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urV1d2DALjQJ"
   },
   "outputs": [],
   "source": [
    "# Standardizing the symptom columns\n",
    "for col in ['Symptom_1', 'Symptom_2', 'Symptom_3', 'Symptom_4', 'Symptom_5', 'Symptom_6', 'Symptom_7', 'Symptom_8', 'Symptom_9', 'Symptom_10', 'Symptom_11', 'Symptom_12', 'Symptom_13', 'Symptom_14', 'Symptom_15', 'Symptom_16', 'Symptom_17']:\n",
    "    dpnlp[col] = dpnlp[col].str.lower().str.strip()\n",
    "\n",
    "#concatenating symptoms into 1 string per row for feature extraction\n",
    "dpnlp['all_symptoms'] = dpnlp[['Symptom_1', 'Symptom_2', 'Symptom_3', 'Symptom_4', 'Symptom_5', 'Symptom_6', 'Symptom_7', 'Symptom_8', 'Symptom_9', 'Symptom_10', 'Symptom_11', 'Symptom_12', 'Symptom_13', 'Symptom_14', 'Symptom_15', 'Symptom_16', 'Symptom_17']].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# preprocessing all symptoms\n",
    "dpnlp['processed_symptoms'] = dpnlp['all_symptoms'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEyncZjyNLVS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Feature Extraction with TFIDF and BioWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCnxClx5NYah"
   },
   "outputs": [],
   "source": [
    "# TFIDF feature extraction\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf.fit_transform(dpnlp['processed_symptoms'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QreiZfg8UzLZ",
    "outputId": "c7fa34f4-0490-467f-c5d1-eb00149fc9d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using gensim library to load BioWordVec\n",
    "bio_word_vec = KeyedVectors.load_word2vec_format('BioWordVec_PubMed_MIMICIII_d200.vec.bin', binary=True)\n",
    "\n",
    "# Test loading\n",
    "print(bio_word_vec['rash'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the word embeddings from Bio-WordVec\n",
    "def get_word_embeddings(word, word_vectors):\n",
    "    embedding_dim = word_vectors.vector_size\n",
    "    return word_vectors[word] if word in word_vectors else np.zeros(embedding_dim)\n",
    "\n",
    "def text_to_embeddings(text, word_vectors):\n",
    "    tokens = text.split()  # Tokenize text\n",
    "    embeddings = [get_word_embeddings(token, word_vectors) for token in tokens]\n",
    "    if embeddings:  # Avoid empty embeddings\n",
    "        return np.mean(embeddings, axis=0)  # Averaging word embeddings\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size) \n",
    "\n",
    "# Applying BioWordVec to transformprocessed text\n",
    "dpnlp['embeddings'] = dpnlp['processed_text'].apply(lambda x: text_to_embeddings(x, bio_word_vec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial results before adding post processing function to deal with the word splitting in tokenization\n",
    "\n",
    "text = \"The patient was diagnosed with diabetes and hypertension.\"\n",
    "Entity: The, Label: 0, Score: 0.999998927116394\n",
    "Entity: patient, Label: 0, Score: 0.999998927116394\n",
    "Entity: was, Label: 0, Score: 0.9999988079071045\n",
    "Entity: diagnosed, Label: 0, Score: 0.9999986886978149\n",
    "Entity: with, Label: 0, Score: 0.9999985694885254\n",
    "Entity: diabetes, Label: B-DISEASE, Score: 0.9999868869781494\n",
    "Entity: and, Label: 0, Score: 0.999997615814209\n",
    "Entity: h, Label: B-DISEASE, Score: 0.9999666213989258\n",
    "Entity: ##yper, Label: B-DISEASE, Score: 0.9349839687347412\n",
    "Entity: ##tens, Label: I-DISEASE, Score: 0.9963584542274475\n",
    "Entity: ##ion, Label: I-DISEASE, Score: 0.9990299940109253\n",
    "Entity: ., Label: 0, Score: 0.9999988079071045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Finetuned TFIDF Logistic regression modelling with BioWord Vec and structured symptoms processed from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preparing the embedding matrix\n",
    "texts = dpnlp['processed_text'].tolist() \n",
    "\n",
    "# Initializing teh tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fitting tokenizer to training data\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Using the tokenizer to get the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "\n",
    "# embedding matrix\n",
    "embedding_dim = 200  \n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Filling the embedding matrix with BioWordVec embeddings\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if word in bio_word_vec:  \n",
    "        embedding_matrix[idx] = bio_word_vec[word]\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yETpL_bNvlv"
   },
   "outputs": [],
   "source": [
    "# Training test split\n",
    "X = X_tfidf\n",
    "y = dpnlp['Disease']  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "id": "Tzc6llUUN_xD",
    "outputId": "ac2fd5d1-4279-4058-8e4f-3d8e5a1bf1aa"
   },
   "outputs": [],
   "source": [
    "## Baseline Log reg modelling with one vs rest classification\n",
    "model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3Gr_jDVOOKQ3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "714fc7a1-3933-4217-9fe1-7decaa01e1d3"
   },
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmzj09nePAc7"
   },
   "source": [
    "Tf-idf feature extraction and Logistic rgression with One-v-rest gives a model with almost perfect accuracy. Further review of the other evaluation metrics shows that the model is predicting excellently for diseases with large sample size, moderately well with an average of 85% for diseases with a sample size of around 30 and not at all (i.e) 0 score for diseases with less than 15 instances in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ANHbgn3O_7z",
    "outputId": "2a8e0c9d-c9c3-4040-9db0-1a19d2405e55"
   },
   "outputs": [],
   "source": [
    "# Saving the TFIDF and finetuned logistic regression model\n",
    "joblib.dump(model, 'b_model.pkl')\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTsHYj-BO_4c"
   },
   "outputs": [],
   "source": [
    "# Testing the model with sample inputs\n",
    "sample_input = \"I have a headache and a high fever\"\n",
    "sample_input_processed = preprocess_text(sample_input)\n",
    "sample_input_tfidf = tfidf.transform([sample_input_processed])\n",
    "sample_input_prediction = model.predict(sample_input_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdUDizI8QBDa",
    "outputId": "6828e322-19e3-481a-8a6b-ad348aa089a6"
   },
   "outputs": [],
   "source": [
    "sample_input_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Feed Forward Neural Network (FFNN) Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training test split FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking the embeddings into a 2D array\n",
    "# Convert the embeddings column to a 2D array\n",
    "X = np.stack(dpnlp['embeddings'].values)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(dpnlp['Disease'])  # Converts disease names into integers\n",
    "\n",
    "# Training Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Handling data imbalance with SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Feed forward neural network (FFNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model class with configurable hyperparameters\n",
    "class DiseasePredictionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_classes=10, dropout=0.3):\n",
    "        super(DiseasePredictionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"hidden_size\": 128,\n",
    "    \"dropout\": 0.3,\n",
    "    \"num_classes\": len(np.unique(y)),  \n",
    "    \"input_size\": X_train.shape[1],  \n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "#  initializing the model\n",
    "model = DiseasePredictionModel(\n",
    "    input_size=hyperparams[\"input_size\"],\n",
    "    hidden_size=hyperparams[\"hidden_size\"],\n",
    "    num_classes=hyperparams[\"num_classes\"],\n",
    "    dropout=hyperparams[\"dropout\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "train_data = torch.tensor(X_train, dtype=torch.float32)\n",
    "train_labels = torch.tensor(y_train, dtype=torch.long)\n",
    "test_data = torch.tensor(X_test, dtype=torch.float32)\n",
    "test_labels = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Creating Dataloaders\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "# Setting device tp use my GPU if it's still got memory or just CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_feedforward(model, train_loader, test_loader, hyperparams):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    model.to(device)  # moving model to the device\n",
    "\n",
    "    for epoch in range(hyperparams[\"epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # moving data to device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_accuracies.append(correct / total)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # moving data to device\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_losses.append(val_loss / len(test_loader))\n",
    "        val_accuracies.append(correct / total)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{hyperparams['epochs']}], \"\n",
    "            f\"Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, \"\n",
    "            f\"Train Acc: {train_accuracies[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Monitoring training and validation loss\n",
    "\n",
    "# Training the Feed Forward model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_feedforward(\n",
    "    model, train_loader, test_loader, hyperparams\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs. Validation Loss (Feed Forward Network)\")\n",
    "plt.legend()\n",
    "plt.savefig(\"feedforward_loss_plot1.png\")  \n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs. Validation Accuracy (Feed Forward Network)\")\n",
    "plt.legend()\n",
    "plt.savefig(\"feedforward_accuracy_plot1.png\")  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rechecking for data imbalance\n",
    "print(dpnlp['Disease'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Evaluating the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Augmentation data for smaller sample diseases to balance the dataset - NLP Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing NLPAug and setting up Augmenters\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Augmenters\n",
    "synonym_augmenter = naw.SynonymAug(aug_src='wordnet')  # Augmenting using WordNet synonyms\n",
    "random_swap_augmenter = naw.RandomWordAug(action=\"swap\")  # Random word swapping\n",
    "random_deletion_augmenter = naw.RandomWordAug(action=\"delete\")  # Random word deletion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming package download\n",
    "print(pos_tag(word_tokenize(\"This is a test sentence\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmenting the Underepresented Diseases - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmenting the underepresented diseases\n",
    "\n",
    "# Filter underrepresented classes\n",
    "underrepresented_classes = ['urinary tract infection', 'varicose veins', 'hypertension', \n",
    "                            'allergy', 'drug reaction', 'diabetes', \n",
    "                            'dimorphic hemorrhoids', 'gastroesophageal reflux disease', \n",
    "                            'peptic ulcer disease']\n",
    "\n",
    "# CDataframe to hold the augmented Data\n",
    "augmented_data = pd.DataFrame()\n",
    "\n",
    "# Augmenting for each underrepresented class\n",
    "for disease in underrepresented_classes:\n",
    "    disease_samples = dpnlp[dpnlp['Disease'] == disease]\n",
    "    augmented_samples = disease_samples.copy()\n",
    "\n",
    "    # Augmenting the 'processed_text' column\n",
    "    augmented_samples['processed_text'] = augmented_samples['processed_text'].apply(\n",
    "        lambda x: synonym_augmenter.augment(x)\n",
    "    )\n",
    "\n",
    "    # Appending it to the augmented dataset\n",
    "    augmented_data = pd.concat([augmented_data, augmented_samples])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(augmented_data[\"Disease\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few entries\n",
    "print(augmented_data['processed_text'].head())\n",
    "\n",
    "# Data types in the columns\n",
    "print(augmented_data['processed_text'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting lists to strings to support augmentation - Cannot augment lists due to error\n",
    "def flatten_and_convert_to_string(entry):\n",
    "    if isinstance(entry, list):\n",
    "        # Flatten the list if necessary and join it into a single string\n",
    "        return ' '.join([str(item) for item in entry])\n",
    "    elif isinstance(entry, str):\n",
    "        return entry  # Leave strings unchanged\n",
    "    else:\n",
    "        return str(entry)  # Convert other data types to strings\n",
    "\n",
    "# Applying the function to the 'processed_text' column\n",
    "augmented_data['processed_text'] = augmented_data['processed_text'].apply(flatten_and_convert_to_string)\n",
    "\n",
    "# Just checking\n",
    "print(augmented_data['processed_text'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types again to ensure they are now strings and properly converted\n",
    "print(augmented_data['processed_text'].apply(type).value_counts())\n",
    "print(augmented_data['processed_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmenting the last 3 Disease classes and the first 6 separately - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Repeating augmentation for underrepresented classes\n",
    "\n",
    "# Ensure NLPAug leverages multiprocessing\n",
    "synonym_augmenter = naw.SynonymAug(aug_src='wordnet', aug_p=0.3) \n",
    "\n",
    "# Checking if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Defining separate groups\n",
    "group_170 = ['urinary tract infection', 'varicose veins', 'hypertension', \n",
    "             'allergy', 'drug reaction', 'diabetes']\n",
    "group_50 = ['dimorphic hemorrhoids', 'gastroesophageal reflux disease', 'peptic ulcer disease']\n",
    "\n",
    "# Target sizes\n",
    "min_sample_size_170 = 3000\n",
    "min_sample_size_50 = 6000\n",
    "\n",
    "# Augmenting Group 1 (170 samples)\n",
    "for disease in group_170:\n",
    "    current_count = len(augmented_data[augmented_data['Disease'] == disease])\n",
    "\n",
    "    while current_count < min_sample_size_170:\n",
    "        new_samples = augmented_data[augmented_data['Disease'] == disease].copy()\n",
    "        new_samples['processed_text'] = new_samples['processed_text'].apply(\n",
    "            lambda x: synonym_augmenter.augment(x)\n",
    "        )\n",
    "        augmented_data = pd.concat([augmented_data, new_samples], ignore_index=True)\n",
    "        current_count = len(augmented_data[augmented_data['Disease'] == disease])\n",
    "        print(f\"Augmented {current_count}/{min_sample_size_170} samples for {disease}.\")\n",
    "\n",
    "# Augmenting Group 2 (50 samples)\n",
    "for disease in group_50:\n",
    "    current_count = len(augmented_data[augmented_data['Disease'] == disease])\n",
    "\n",
    "    while current_count < min_sample_size_50:\n",
    "        new_samples = augmented_data[augmented_data['Disease'] == disease].copy()\n",
    "        new_samples['processed_text'] = new_samples['processed_text'].apply(\n",
    "            lambda x: synonym_augmenter.augment(x)\n",
    "        )\n",
    "        augmented_data = pd.concat([augmented_data, new_samples], ignore_index=True)\n",
    "        current_count = len(augmented_data[augmented_data['Disease'] == disease])\n",
    "        print(f\"Augmented {current_count}/{min_sample_size_50} samples for {disease}.\")\n",
    "\n",
    "print(\"Data augmentation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the last 3 diseases having the minmum number of samples, the while loop is playing catch up by over augmenting the larger classes to try to get the smallest classes up to the mimimum level. To resolve this, I have decided to augment each group of classes separately. The classes with 170 samples will be augmented together, and so will the samples with only 50. That way, both are augmented separately and no one is oversampled in favor of the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(augmented_data['Disease'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to avoid losing augmented data\n",
    "augmented_data.to_csv('augmented_data_backup.csv', index=False)\n",
    "print(\"Data augmentation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the original and augmented datasets without dropping duplicates\n",
    "final_data = pd.concat([dpnlp, augmented_data], ignore_index=True)\n",
    "\n",
    "# strings\n",
    "final_data['processed_text'] = final_data['processed_text'].apply(\n",
    "    lambda x: ' '.join(x) if isinstance(x, (list, np.ndarray)) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the combined dataset to a CSV file\n",
    "file_path = \"final_Balanced_Augmented Datasset.csv\" \n",
    "final_data.to_csv(file_path, index=False)\n",
    "print(f\"Combined dataset saved at {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the augmented data\n",
    "final_data = pd.read_csv(\"final_Balanced_Augmented Datasset.csv\")\n",
    "\n",
    "# Convert the disease descriptions in the 'text' column to lowercase\n",
    "final_data[\"text\"] = final_data[\"text\"].str.lower()\n",
    "\n",
    "# Updated dataframe\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['Disease'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading bioWordVec embeddings\n",
    "# Load BioWordVec\n",
    "biowordvec = KeyedVectors.load_word2vec_format(\"BioWordVec_PubMed_MIMICIII_d200.vec.bin\", binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating text embeddings\n",
    "def embed_text(text, embedding_model):\n",
    "    words = text.split()  # Split input text into words\n",
    "    word_vectors = [embedding_model[word] for word in words if word in embedding_model]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)  \n",
    "    else:\n",
    "        return np.zeros(embedding_model.vector_size)  # Fallback for missing words\n",
    "\n",
    "# Adding embeddings\n",
    "final_data['embeddings'] = final_data['text'].apply(lambda x: embed_text(x, biowordvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1:  Dense Neural Network with BioWordVec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = np.stack(final_data[\"embeddings\"])\n",
    "y = pd.get_dummies(final_data[\"Disease\"]).values\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#  Model\n",
    "b_model = Sequential([\n",
    "    Input(shape=(X.shape[1],)),  \n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(y.shape[1], activation=\"softmax\")  \n",
    "])\n",
    "\n",
    "b_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Evaluateing the model on the test set\n",
    "b_model.evaluate(X_test, y_test)\n",
    "\n",
    "#Adding class weights to balance the importance of the minority classes\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(final_data[\"Disease\"]), y=final_data[\"Disease\"])\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Model training\n",
    "history = b_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=12,\n",
    "    class_weight=class_weights_dict\n",
    ")\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "b_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "\n",
    "# Test set predictions\n",
    "y_pred = b_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  \n",
    "y_true = np.argmax(y_test, axis=1) \n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes))\n",
    "\n",
    "# OConfusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making disease predictions with teh model\n",
    "\n",
    "def predict_disease(input_text, model, embedding_model, class_names):\n",
    "    # Preprocess the input text\n",
    "    input_embedding = embed_text(input_text, embedding_model)\n",
    "    input_embedding = np.expand_dims(input_embedding, axis=0)   \n",
    "\n",
    "    # Predicting probabilities\n",
    "    predictions = model.predict(input_embedding)\n",
    "\n",
    "    # Top 5 predictions\n",
    "    top_indices = predictions[0].argsort()[-5:][::-1]  \n",
    "    top_diseases = [(class_names[idx], predictions[0][idx]) for idx in top_indices]\n",
    "\n",
    "    return top_diseases\n",
    "\n",
    "# Class names\n",
    "class_names = pd.get_dummies(final_data[\"Disease\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing function with sample input\n",
    "input_text = \"The past few days I havent been feeling well. My head has a full feeling. And my sinuses are all congested. I cant go 10 minutes without coughing or sneezing. \"\n",
    "top_diseases = predict_disease(input_text, b_model, biowordvec, class_names)\n",
    "\n",
    "# Predictions\n",
    "print(\"Top predicted diseases with probabilities:\")\n",
    "for disease, prob in top_diseases:\n",
    "    print(f\"{disease}: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input test 5\n",
    "input_text = \"I have been having strange rashes all oer my body with some having round blusters. They are not itchy, just red and around my abdomen. I am always tired and I cant sleep well\"\n",
    "top_diseases = predict_disease(input_text, b_model, biowordvec, class_names)\n",
    "\n",
    "#   Print the predictions\n",
    "print(\"Top predicted diseases with probabilities:\")\n",
    "for disease, prob in top_diseases:\n",
    "    print(f\"{disease}: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample input test 2 - This result is a little iffy. How did we get hyperytension from that?\n",
    "input_text = \"I am getting a severe headache and i am unable to sleep \"\n",
    "top_diseases = predict_disease(input_text, b_model, biowordvec, class_names)\n",
    "\n",
    "#   Print the predictions\n",
    "print(\"Top predicted diseases with probabilities:\")\n",
    "for disease, prob in top_diseases:\n",
    "    print(f\"{disease}: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input test 3\n",
    "input_text = \"I had few blood clot patches on my legs. It happens with me when I'm feeling weak or I had few foot cramps during winter.\"\n",
    "top_diseases = predict_disease(input_text, b_model, biowordvec, class_names)\n",
    "\n",
    "#   Print the predictions\n",
    "print(\"Top predicted diseases with probabilities:\")\n",
    "for disease, prob in top_diseases:\n",
    "    print(f\"{disease}: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input test 4\n",
    "input_text = \"ts just a normal cough and fever\"\n",
    "top_diseases = predict_disease(input_text, b_model, biowordvec, class_names)\n",
    "\n",
    "#   Print the predictions\n",
    "print(\"Top predicted diseases with probabilities:\")\n",
    "for disease, prob in top_diseases:\n",
    "    print(f\"{disease}: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Based on the outputs of all the test samples, it looks to me like descriptions that are more verbose or fuller tend to have better predictions that closely match the predictions of Medical chatGPT. While all the predictions seem possible, some are more likely than others. Shorter descriptions look like they have far fetched predictions. For instance, even though I know hypertension can be a likely disease for sample input 2, it's too little information to predict hypertension. Overall, the model performs much better with more information provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model 2: Experimental Zero-Shot learning with Disease ontology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing SPARQLWrapper to access the disease ontology\n",
    "#!pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Removing special characters\n",
    "    return text.strip()\n",
    "\n",
    "# Function to compute sentence embeddings\n",
    "def sentence_embedding(sentence, word2index, embedding_matrix):\n",
    "    words = sentence.split()\n",
    "    indices = [word2index[word] for word in words if word in word2index]\n",
    "    if indices:\n",
    "        \n",
    "        # moving embeddings for this sentence to GPU\n",
    "        embeddings = embedding_matrix[indices].to(\"cuda\")\n",
    "        return embeddings.mean(dim=0)  \n",
    "    else:\n",
    "        return torch.zeros(embedding_matrix.shape[1], device=\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "#  initializing SPARQL endpoint\n",
    "sparql = SPARQLWrapper(\"https://sparql.disease-ontology.org/\")\n",
    "sparql.setQuery(\"\"\"\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "PREFIX obo: <http://purl.obolibrary.org/obo/>\n",
    "PREFIX oboInOwl: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "\n",
    "SELECT ?id ?label ?source\n",
    "WHERE {\n",
    "  ?iri a owl:Class ;\n",
    "       oboInOwl:id ?id ;\n",
    "       rdfs:label ?label ;\n",
    "       obo:IAO_0000115 ?definition ;\n",
    "       oboInOwl:hasOBONamespace \"disease_ontology\" .\n",
    "\n",
    "  # Ignore obsolete\n",
    "  FILTER NOT EXISTS { ?iri owl:deprecated ?deprecated . }\n",
    "\n",
    "  [] owl:annotatedSource ?iri ;\n",
    "     owl:annotatedProperty obo:IAO_0000115 ;\n",
    "     owl:annotatedTarget ?definition ;\n",
    "     oboInOwl:hasDbXref ?def_src .\n",
    "\n",
    "  BIND(REPLACE(?def_src, \"url:\", \"\") AS ?source)\n",
    "}\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "# Results\n",
    "results = sparql.query().convert()\n",
    "\n",
    "# DataFrame - Saves my time\n",
    "data = []\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    data.append({\n",
    "        \"Disease ID\": result[\"id\"][\"value\"],\n",
    "        \"Disease Name\": result[\"label\"][\"value\"],\n",
    "        \"Source\": result[\"source\"][\"value\"]\n",
    "    })\n",
    "\n",
    "disease_descriptions = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!pip install accelerate transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Loading BioWordVec and preparing embeddings\n",
    "#biowordvec already in enviromnet - I.e the word vectors\n",
    "embedding_matrix = torch.tensor(biowordvec.vectors, device=\"cpu\")\n",
    "word2index = {word: idx for idx, word in enumerate(biowordvec.index_to_key)}\n",
    "\n",
    "# Adding embeddings to disease ontology\n",
    "disease_descriptions['embedding'] = disease_descriptions['Disease Name'].apply(\n",
    "    lambda x: sentence_embedding(x, word2index, embedding_matrix)\n",
    ")\n",
    "\n",
    "# Dataset\n",
    "final_data = pd.read_csv(\"final_Balanced_Augmented Datasset.csv\")\n",
    "\n",
    "#filtering non_string values\n",
    "final_data['processed_symptoms'] = final_data['processed_symptoms'].apply(\n",
    "    lambda x: str(x) if not isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# handling missing values\n",
    "final_data['processed_symptoms'] = final_data['processed_symptoms'].fillna(\"\")\n",
    "\n",
    "symptom_descriptions = final_data['processed_symptoms'].unique()\n",
    "symptom_descriptions = symptom_descriptions.dropna() #filtering out Nan Values\n",
    "\n",
    "symptom_embeddings = torch.stack(\n",
    "    [sentence_embedding(preprocess_text(symptom), word2index, embedding_matrix)\n",
    "     for symptom in symptom_descriptions]\n",
    ")\n",
    "\n",
    "# Generating embeddings for symptoms\n",
    "symptom_embeddings = torch.stack(\n",
    "    [sentence_embedding(symptom, word2index, embedding_matrix) for symptom in symptom_descriptions]\n",
    ")\n",
    "\n",
    "#  Cosine similarities\n",
    "disease_embedding_matrix = torch.stack(disease_descriptions['embedding'].tolist())\n",
    "similarities = torch.nn.functional.cosine_similarity(\n",
    "    symptom_embeddings.unsqueeze(1),  # Reshape for broadcasting\n",
    "    disease_embedding_matrix.unsqueeze(0),\n",
    "    dim=-1,\n",
    ")\n",
    "\n",
    "# Smptoms to disease matching\n",
    "top_k = 5  \n",
    "for i, symptom in enumerate(symptom_descriptions):\n",
    "    top_indices = torch.topk(similarities[i], top_k).indices\n",
    "    top_diseases = disease_descriptions.iloc[top_indices.tolist()]['Disease Name']\n",
    "    print(f\"Symptom: '{symptom}' -> Top Diseases: {list(top_diseases)}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Wh6N7cao3qLv",
    "QIduOq_XYKi6",
    "zT03zw1K1Rs2"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0283758a4f004b018614a856e5b6bbc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07417350a9fc4f5bb23ee5fe14845012": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14a15397b64441debecf4a9a1df740c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23f523e623504dc1be1501b1381c34cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1bf2d3522b149649c5474f30334ab81",
      "placeholder": "",
      "style": "IPY_MODEL_07417350a9fc4f5bb23ee5fe14845012",
      "value": "pytorch_model.bin:100%"
     }
    },
    "323b069f93eb422297df4bffda943d2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3493d89bfea7477eb29797f772affd4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e83fb12a3de4425b91a6172234cd20d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bfb74df5cd474a13bbd63396ae58592d",
      "placeholder": "",
      "style": "IPY_MODEL_e5b4b8d5099a4d57967f37e1c7946be8",
      "value": "436M/436M[00:02&lt;00:00,156MB/s]"
     }
    },
    "47e492181bf545e6810785247b7e2f35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4ff1541ce8c4628beb26d5ddd4a31b8",
      "placeholder": "",
      "style": "IPY_MODEL_14a15397b64441debecf4a9a1df740c4",
      "value": "385/385[00:00&lt;00:00,25.1kB/s]"
     }
    },
    "608338c062c6454d8d762d624ea93bc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b41d6af37c8e42c8898a43fed1a9f86f",
      "placeholder": "",
      "style": "IPY_MODEL_a44eb9f6941d4499a43c2039ed790bfe",
      "value": "config.json:100%"
     }
    },
    "68c130f410bd439fb834dc60e6b41e8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7065f93cccaf4049bbbb2cb53f2eeed8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be545be4e0b34a1a9ef927213627b8ea",
      "max": 435778770,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68c130f410bd439fb834dc60e6b41e8d",
      "value": 435778770
     }
    },
    "7245f2b191cf404f9ba7a7a1efd3069e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "756e1e229322432ebcf3ef3c4a14376c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7983a0e3159746e3baae0b677565fa03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "799934c5a7e644ef9388458eeb297338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ce50733a79a4555a494680591d7855b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8541038adac24d98a9486c67672824fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7983a0e3159746e3baae0b677565fa03",
      "max": 385,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_921923b2ce0149b5977198888a06b1c6",
      "value": 385
     }
    },
    "921923b2ce0149b5977198888a06b1c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a399d624c022435e987625aec96543d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_608338c062c6454d8d762d624ea93bc2",
       "IPY_MODEL_8541038adac24d98a9486c67672824fe",
       "IPY_MODEL_47e492181bf545e6810785247b7e2f35"
      ],
      "layout": "IPY_MODEL_756e1e229322432ebcf3ef3c4a14376c"
     }
    },
    "a44eb9f6941d4499a43c2039ed790bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa83070ac49844a09bf911f09916fba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_323b069f93eb422297df4bffda943d2c",
      "placeholder": "",
      "style": "IPY_MODEL_0283758a4f004b018614a856e5b6bbc2",
      "value": "213k/213k[00:00&lt;00:00,452kB/s]"
     }
    },
    "b1bf2d3522b149649c5474f30334ab81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b41d6af37c8e42c8898a43fed1a9f86f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6432849c76743f7a33f9d36ab06bb7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23f523e623504dc1be1501b1381c34cb",
       "IPY_MODEL_7065f93cccaf4049bbbb2cb53f2eeed8",
       "IPY_MODEL_3e83fb12a3de4425b91a6172234cd20d"
      ],
      "layout": "IPY_MODEL_e3911abf2413470eb7820bc7ca0a461d"
     }
    },
    "be545be4e0b34a1a9ef927213627b8ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfb74df5cd474a13bbd63396ae58592d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4ff1541ce8c4628beb26d5ddd4a31b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6346cf22ef941d9897e453c27cb361f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb7b0f6d1bd14e219b52da3ca88bab64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_799934c5a7e644ef9388458eeb297338",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3493d89bfea7477eb29797f772affd4e",
      "value": 213450
     }
    },
    "e3911abf2413470eb7820bc7ca0a461d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5b4b8d5099a4d57967f37e1c7946be8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea927e7c9632497e90bd821fd5a0bfb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7245f2b191cf404f9ba7a7a1efd3069e",
      "placeholder": "",
      "style": "IPY_MODEL_7ce50733a79a4555a494680591d7855b",
      "value": "vocab.txt:100%"
     }
    },
    "f9a27516b937452387bc7048daa7460c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ea927e7c9632497e90bd821fd5a0bfb0",
       "IPY_MODEL_cb7b0f6d1bd14e219b52da3ca88bab64",
       "IPY_MODEL_aa83070ac49844a09bf911f09916fba6"
      ],
      "layout": "IPY_MODEL_c6346cf22ef941d9897e453c27cb361f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
